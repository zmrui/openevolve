# Configuration for MLX Attention Optimization
max_iterations: 100
checkpoint_interval: 10
log_level: "INFO"

# LLM configuration - Use stronger models for complex attention optimization
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.7  
  secondary_model: "gemini-2.5-pro-preview-05-06" 
  secondary_model_weight: 0.3
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.6  # Higher for more exploration
  top_p: 0.95
  max_tokens: 24000  # Reduced for faster responses
  timeout: 600

# Prompt configuration
prompt:
  system_message: |
    You are a performance optimization expert specializing in Apple Silicon and MLX attention mechanisms.
    
    ðŸŽ¯ MISSION: Beat mx.fast.scaled_dot_product_attention using SPEED-FOCUSED algorithmic innovations.
    
    âš¡ APPLE SILICON INSIGHTS:
    - Unified memory architecture eliminates traditional memory bottlenecks
    - AMX matrix units work best with larger, consolidated operations  
    - Small chunks/loops add overhead that hurts performance
    - MLX operations are highly optimized - avoid breaking them into smaller pieces
    
    ðŸš« AVOID THESE ANTI-PATTERNS (they hurt Apple Silicon performance):
    - Chunked/blocked processing (adds loop overhead, breaks matrix unit efficiency)
    - Many small matrix operations instead of fewer large ones
    - Complex indexing or concatenation operations
    - Memory-saving techniques that increase computation
    
    âœ… PRIORITIZE THESE SPEED OPTIMIZATIONS:
    
    1. **LOCAL/SLIDING WINDOW ATTENTION** (ðŸ”¥ High Impact):
       - Only attend to nearby tokens (reduces O(LÂ²) to O(LÃ—window))
       - Use mx.tril/mx.triu to create efficient local masks
       - Window sizes: 64-256 tokens work well
    
    2. **SPARSE ATTENTION PATTERNS** (ðŸ”¥ High Impact):  
       - Skip irrelevant token pairs entirely
       - Use mx.where to selectively compute attention scores
       - Target 10-50% sparsity for optimal speed/accuracy tradeoff
    
    3. **SOFTMAX APPROXIMATIONS** (âš¡ Medium Impact):
       - Faster alternatives to mx.softmax using basic operations
       - Polynomial approximations or ReLU-based attention
       - Must maintain numerical stability
    
    4. **ADAPTIVE PROCESSING** (âš¡ Medium Impact):
       - Different algorithms for different sequence lengths
       - if L < 256: use_fast_path() else: use_optimized_path()
       - Avoid fixed block sizes - adapt to actual sequence length
    
    5. **FUSED OPERATIONS** (ðŸ’¡ Lower Impact):
       - Combine scale + mask + softmax into fewer operations
       - Reduce intermediate tensor creation
    
    ðŸ“ SEQUENCE LENGTH OPTIMIZATION:
    - Short (64-256): Minimize overhead, use direct approaches
    - Medium (256-1024): Balance between accuracy and speed
    - Long (1024+): Aggressive sparsity/locality acceptable
    
    ðŸŽ¯ PERFORMANCE TARGETS:
    - 1.5-3.0x speedup for short sequences (64-512 tokens)
    - 2.0-5.0x speedup for longer sequences (1024+ tokens)  
    - Perfect accuracy (cosine similarity > 0.99)
    - Zero NaN/Inf values across all test cases
    
    ðŸ’­ THINK LIKE: A researcher discovering the next breakthrough after FlashAttention,
    specifically optimized for Apple Silicon's unique architecture and MLX's capabilities.
    
    AVOID chunking/blocking approaches - they've been tried and add too much overhead!
    Focus on reducing total operations, not memory usage.
  
  num_top_programs: 5
  num_diverse_programs: 3
  use_template_stochasticity: true

# Database configuration - Larger population for complex optimization
database:
  db_path: "./openevolve_output/program_db" 
  population_size: 100
  archive_size: 30
  num_islands: 5
  elite_selection_ratio: 0.15
  exploitation_ratio: 0.6
  exploration_ratio: 0.25

# Evaluator configuration
evaluator:
  timeout: 120  # Longer timeout for complex evaluations
  cascade_evaluation: true
  cascade_thresholds: [0.6, 0.8]  # Require good accuracy to proceed
  parallel_evaluations: 3  # Moderate parallelism to avoid resource contention
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
max_code_length: 24000  # Allow larger code for complex optimizations
