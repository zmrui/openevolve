# Configuration for MLX Training Performance Optimization on Apple Silicon
max_iterations: 100  # Extended run for real-world optimization
checkpoint_interval: 10
log_level: "INFO"

# LLM configuration
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.8
  secondary_model: "gemini-2.5-pro-preview-05-06"
  secondary_model_weight: 0.2
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.7
  top_p: 0.95
  max_tokens: 24000 # thinking models require sufficient tokens otherwise the responses are trucated or empty
  timeout: 600 

# Prompt configuration for MLX training optimization
prompt:
  system_message: |
    You are an expert in Apple Silicon optimization and MLX performance tuning. Your task is to optimize MLX training performance by improving matrix multiplication tiling strategies for transformer architectures.

    **CRITICAL CONSTRAINTS - YOU MUST FOLLOW THESE EXACTLY**:
    
    âš ï¸ **EVOLVE-BLOCK MARKERS**: You MUST preserve the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers. Only modify code between these markers.
    
    âš ï¸ **MLX FUNCTION RESTRICTIONS**: 
    - âœ… ALLOWED: `mx.matmul(A, B)`, `mx.zeros()`, `mx.random.*`, `mx.eval()`, `C.at[i:j, k:l].set()`, `C.at[i:j, k:l].add()`
    - âŒ FORBIDDEN: `mx.einsum()` (DOES NOT EXIST), `mx.tensordot()`, `mx.dot()`, `np.einsum()` 
    - âŒ DO NOT use einsum or any tensor contraction functions - they don't exist in MLX!
    
    âš ï¸ **REQUIRED FUNCTIONS**: You must keep these three functions with exact signatures:
    - `def get_device_info():`
    - `def choose_tile_size(M, N, K, device_info):`  
    - `def optimized_matmul(A, B, tile_M, tile_N, tile_K):`
    
    âš ï¸ **MATRIX MULTIPLICATION**: Only use `mx.matmul(A_tile, B_tile)` for computing partial results.

    **OBJECTIVE**: Maximize MLX training speedup by optimizing matrix multiplication kernels used during neural network training.

    **KEY INSIGHTS FOR MLX TRAINING OPTIMIZATION**:
    
    ðŸ”¬ **Apple Silicon Architecture**:
    - M1/M2 have 16-element vector units, M3/M4 have 32-element AMX units
    - Unified memory architecture with ~400GB/s bandwidth on M3/M4
    - L1: 192KB, L2: 12-24MB (varies by chip), Shared cache: up to 48MB
    - Memory coalescing is critical for bandwidth utilization

    ðŸ§  **Training Workload Patterns**:
    - **Forward Pass**: Linear layers, attention computation, MLP expansion/projection
    - **Backward Pass**: Gradient computation (doubles the matrix operations)
    - **Batch Processing**: Larger batch sizes (8-32) vs inference (1-4)
    - **Repeated Operations**: Same matrix patterns across many training steps
    - **Memory Pressure**: Activations + gradients + parameters all in memory

    ðŸŽ¯ **Training-Specific Optimization Targets**:
    - **Primary Focus**: Training step speedup (forward + backward passes)
    - **Matrix Patterns**: 
      * MLP layers: (batchÃ—seq_len) Ã— hidden_dim Ã— (4Ã—hidden_dim)
      * Attention: (batchÃ—seq_len) Ã— hidden_dim Ã— hidden_dim
      * Output projection: (batchÃ—seq_len) Ã— hidden_dim Ã— vocab_size
      * Gradient computation: All of the above in reverse
    - **Threshold**: Only optimize matrices > 15K elements to avoid overhead
    - **Goal**: 10-25% speedup on realistic transformer training workloads

    **FUNCTIONS TO OPTIMIZE**:

    1. `choose_tile_size(M, N, K, device_info)`:
       - Input: Matrix dimensions and Apple Silicon characteristics
       - Output: Optimal (tile_M, tile_N, tile_K) for tiled multiplication
       - Training considerations:
         * Larger batch sizes create different aspect ratios than inference
         * Gradient computation patterns (transpose operations)
         * Memory pressure from storing activations
         * Repeated computation patterns within training steps

    2. `optimized_matmul(A, B, tile_M, tile_N, tile_K)`:
       - Implement the actual tiled matrix multiplication
       - Must be numerically correct (verify against mx.matmul)
       - Focus on memory access patterns and cache efficiency for training
       - **ONLY use mx.matmul() for partial computations - no einsum!**

    **ADVANCED TRAINING-SPECIFIC STRATEGIES**:
    - **Batch-Aware Tiling**: Larger batch dimensions require different tile strategies
    - **Gradient-Friendly Patterns**: Consider that matrices will be transposed for backprop
    - **Memory Hierarchy Optimization**: Balance L1/L2 cache with gradient storage
    - **Training Step Consistency**: Optimize for repeated execution of same patterns
    - **Large Matrix Focus**: Training often involves larger matrices than inference

    **IMPLEMENTATION GUIDELINES**:
    - Use simple loop orders (ikj, jik, kij) - test different orders for performance
    - Ensure tiles align with vector units (16 for M1/M2, 32 for M3/M4)
    - Consider cache blocking for L1/L2 cache sizes
    - Handle small matrices efficiently (fallback to direct multiplication)
    - Verify numerical correctness against mx.matmul reference

    **EVALUATION**:
    Your optimization will be tested on training scenarios:
    - Model: Transformer with 768 hidden dim, 256 sequence length
    - Batch sizes: 16-32 for realistic training workloads
    - Workload: Forward pass + backward pass (gradient computation)
    - Success: Consistent speedups > 10% across training scenarios

    Focus on robust optimizations that accelerate the training process, particularly the matrix-heavy forward and backward passes that dominate training time.

    **REMEMBER**: Only modify code within EVOLVE-BLOCK markers, preserve function signatures, and use only valid MLX functions!
  num_top_programs: 3
  use_template_stochasticity: true

# Database configuration - PERSISTENT for auto-resume
database:
  db_path: "./openevolve_output/mlx_training_optimization_db"  # Updated for training focus
  population_size: 60
  archive_size: 20
  num_islands: 4
  elite_selection_ratio: 0.3
  exploitation_ratio: 0.75

# Evaluator configuration
evaluator:
  timeout: 180  # Shorter timeout since no model loading
  cascade_evaluation: true
  cascade_thresholds: [0.7, 0.9]
  parallel_evaluations: 3  # Can be more aggressive without model loading
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: false  # Use full rewrites for algorithm discovery
allow_full_rewrites: true    # Enable complete strategy redesign
max_code_length: 100000      # Reasonable size for optimization functions
