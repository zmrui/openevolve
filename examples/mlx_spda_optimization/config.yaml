# Configuration for Custom Metal Kernel Evolution
# Focus: Evolve Metal C++ kernel source code for block-diagonal attention

max_iterations: 80
checkpoint_interval: 5
log_level: "INFO"

# LLM configuration
llm:
  primary_model: "gemini-2.5-flash-preview-05-20"
  primary_model_weight: 0.7
  secondary_model: "gemini-2.5-pro-preview-05-06"
  secondary_model_weight: 0.3
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  temperature: 0.8
  top_p: 0.9
  max_tokens: 24000
  timeout: 600

# Focused prompt for Metal kernel optimization
prompt:
  system_message: |
    ðŸŽ¯ **MISSION: Evolve High-Performance Metal Kernel for Block-Diagonal Attention**
    
    You are evolving the **Metal C++ kernel source code** that computes block-diagonal attention
    for packed sequences. Your goal is to beat `mx.fast.scaled_dot_product_attention` by 
    optimizing computation patterns and memory access.
    
    ## **THE EVOLUTION TARGET**
    
    **SINGLE EVOLUTION BLOCK**: The Metal C++ kernel source code inside the `kernel_source` string.
    
    ```cpp
    template<typename T>
    [[kernel]] void block_diagonal_attention(
        const device T* queries [[buffer(0)]],
        const device T* keys [[buffer(1)]],
        const device T* values [[buffer(2)]],
        const device bool* mask [[buffer(3)]],
        const device float* scale_ptr [[buffer(4)]],
        device T* output [[buffer(5)]],
        // ... thread parameters
    ) {
        // THIS IS WHAT YOU EVOLVE - the Metal C++ implementation
        // Current: Basic implementation that processes each query position
        // Goal: Optimized kernel that outperforms mx.fast.scaled_dot_product_attention
    }
    ```
    
    ## **WHY BLOCK-DIAGONAL SHOULD WIN**
    
    **The Advantage**: Standard SPDA computes attention for ALL positions then masks out unwanted ones.
    Block-diagonal attention can skip masked computations entirely, saving:
    - 50-95% of compute (depending on sparsity)
    - Memory bandwidth on masked regions
    - Cache pollution from unused data
    
    **Test Scenarios**: You'll be evaluated on packed sequences where 50-95% of the attention 
    matrix is masked (wasted computation in standard SPDA).
    
    ## **METAL OPTIMIZATION OPPORTUNITIES**
    
    ### ðŸš€ **HIGH IMPACT** (Focus here first!)
    
    **1. Skip Masked Computations**
    ```cpp
    // Instead of computing all then masking:
    for (uint key_pos = 0; key_pos < L; key_pos++) {
        if (!mask[mask_base + key_pos]) continue;  // SKIP entirely
        // Only compute for valid positions
    }
    ```
    
    **2. Optimize Memory Access Patterns**
    ```cpp
    // Vectorized loads where possible
    // Coalesced memory access
    // Minimize memory bandwidth usage
    ```
    
    **3. Thread Utilization**
    ```cpp
    // Better thread assignment
    // Reduce thread divergence 
    // Balance workload across threads
    ```
    
    ### âš¡ **MEDIUM IMPACT** 
    
    **4. Algorithmic Improvements**
    ```cpp
    // Fused operations (score + softmax + output)
    // Reduced intermediate storage
    // Optimized softmax computation
    ```
    
    **5. Apple Silicon Specific**
    ```cpp
    // Leverage unified memory architecture
    // Optimize for Apple GPU characteristics
    // Use Metal-specific features effectively
    ```
    
    ### ðŸ”§ **LOW IMPACT** (Polish)
    
    **6. Code Structure**
    ```cpp
    // Loop unrolling where beneficial
    // Register optimization
    // Instruction scheduling
    ```
    
    ## **CRITICAL CONSTRAINTS**
    
    **âœ… KEEP THESE UNCHANGED**:
    - Kernel signature and buffer layout
    - Template parameters and grid/threadgroup setup
    - Overall algorithm structure (attention computation)
    
    **ðŸŽ¯ EVOLVE THESE**:
    - Memory access patterns and vectorization
    - Thread assignment and workload distribution  
    - Computation ordering and fusion
    - Optimization of inner loops
    - Use of Metal-specific features
    
    **âŒ AVOID THESE ERRORS**:
    - Changing buffer indices or parameter types
    - Breaking the attention mathematical correctness
    - Using undefined Metal features or syntax
    - Complex control flow that causes thread divergence
    
    ## **SUCCESS METRICS**
    
    **Correctness** (Must achieve):
    - âœ… 75%+ test pass rate (MSE < 1e-3 vs reference)
    - âœ… No NaN/Inf outputs
    - âœ… Correct output shapes
    
    **Performance** (Optimization targets):
    - ðŸŽ¯ **1.2x+ speedup** over mx.fast.scaled_dot_product_attention (good)
    - ðŸŽ¯ **1.5x+ speedup** over SPDA (excellent)
    - ðŸŽ¯ **2.0x+ speedup** over SPDA (outstanding)
    - ðŸŽ¯ Consistent gains across sparse patterns
    
    ## **EVALUATION SCENARIOS**
    
    You'll be tested on increasingly sparse block-diagonal patterns:
    - **50% sparse**: 2 large blocks (moderate advantage expected)
    - **75% sparse**: 4 medium blocks (good advantage expected)  
    - **87.5% sparse**: 8 small blocks (large advantage expected)
    - **93.75% sparse**: 16 tiny blocks (massive advantage expected)
    
    The sparser the pattern, the more your optimized kernel should outperform SPDA!
    
    ## **METAL PROGRAMMING TIPS**
    
    **Memory Access**:
    ```cpp
    // Good: Sequential access
    const device T* ptr = queries + base_idx;
    for (uint d = 0; d < D; d++) { score += ptr[d] * other[d]; }
    
    // Good: Vectorized access (when aligned)
    float4 q_vec = *((device float4*)(queries + base_idx));
    ```
    
    **Thread Efficiency**:
    ```cpp
    // Good: Minimize thread divergence
    if (condition_same_for_threadgroup) {
        // All threads take same path
    }
    
    // Good: Balance workload
    for (uint i = thread_id; i < work_items; i += num_threads) {
        // Even distribution
    }
    ```
    
    **Computation Optimization**:
    ```cpp
    // Good: Minimize recomputation
    float score = precomputed_base + delta;
    
    // Good: Fuse operations
    output[i] = T(softmax_weight * value + bias);
    ```
    
    ## **EXAMPLE OPTIMIZATIONS TO CONSIDER**
    
    ```cpp
    // 1. Skip masked regions entirely
    if (!mask[mask_idx]) continue;
    
    // 2. Vectorize inner loops  
    for (uint d = 0; d < D; d += 4) {
        float4 q_chunk = *((device float4*)(q_ptr + d));
        float4 k_chunk = *((device float4*)(k_ptr + d));
        score += dot(q_chunk, k_chunk);
    }
    
    // 3. Optimize thread assignment
    uint items_per_thread = (actual_work + num_threads - 1) / num_threads;
    uint start_idx = thread_id * items_per_thread;
    
    // 4. Reduce memory pressure
    // Store frequently accessed values in registers
    float scale_cached = scale_ptr[0];
    ```
    
    Remember: Focus on the **biggest wins first** - skipping masked computations and 
    optimizing memory access will have much more impact than micro-optimizations!
  
  num_top_programs: 4
  num_diverse_programs: 2
  use_template_stochasticity: true

# Database configuration
database:
  db_path: "./openevolve_output/program_db"
  population_size: 50
  archive_size: 20
  num_islands: 3
  elite_selection_ratio: 0.20
  exploitation_ratio: 0.60
  exploration_ratio: 0.20

# Evaluator configuration  
evaluator:
  timeout: 600
  cascade_evaluation: true
  cascade_thresholds: [0.6, 0.75]
  parallel_evaluations: 1
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
max_code_length: 25000
